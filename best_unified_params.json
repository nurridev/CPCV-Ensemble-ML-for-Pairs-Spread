{
  "scaler_type": "minmax",
  "stride": 23,
  "train_split": 0.7311989040672405,
  "n_encoder_layers": 2,
  "first_layer_units": 224,
  "first_activation": "tanh",
  "regularization_type": "l1_l2",
  "l1_strength": 0.0001461896279370495,
  "l2_strength": 0.0028016351587162596,
  "use_batch_norm": false,
  "use_dropout": false,
  "layer_1_strategy": "decrease",
  "layer_1_activation": "leaky_relu",
  "layer_1_batch_norm": true,
  "layer_1_dropout": true,
  "layer_1_dropout_rate": 0.14881529393791154,
  "latent_dim": 34,
  "latent_activation": "gelu",
  "decoder_1_units": 284,
  "decoder_1_activation": "gelu",
  "decoder_1_batch_norm": false,
  "decoder_1_dropout": false,
  "decoder_0_batch_norm": true,
  "decoder_0_dropout": true,
  "decoder_0_dropout_rate": 0.2123738038749523,
  "output_activation": "tanh",
  "optimizer": "adam",
  "learning_rate": 0.018274508859816022,
  "loss_function": "huber",
  "batch_size": 56,
  "epochs": 67,
  "patience": 45,
  "use_lr_scheduler": true,
  "lr_patience": 6,
  "lr_factor": 0.3176876252009636
}